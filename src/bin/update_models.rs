//! Binary to fetch model capabilities from OpenRouter and update generated.rs.
//!
//! Run with: cargo run --bin update-models
//!
//! This is called by GitHub Actions on a schedule to keep the model lists up to date.

use serde::Deserialize;
use std::collections::HashSet;
use std::fs;
use std::io::Write;

const OPENROUTER_API: &str = "https://openrouter.ai/api/v1/models";
const OUTPUT_FILE: &str = "src/generated.rs";

#[derive(Debug, Deserialize)]
struct ApiResponse {
    data: Vec<Model>,
}

#[derive(Debug, Deserialize)]
struct Model {
    id: String,
    #[serde(default)]
    architecture: Option<Architecture>,
}

#[derive(Debug, Deserialize)]
struct Architecture {
    #[serde(default)]
    input_modalities: Vec<String>,
    #[serde(default)]
    output_modalities: Vec<String>,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    println!("Fetching models from OpenRouter API...");

    let response = reqwest::blocking::get(OPENROUTER_API)?;
    let api_response: ApiResponse = response.json()?;

    println!("Found {} models", api_response.data.len());

    let mut vision_models = HashSet::new();
    let mut text_only_models = HashSet::new();
    let mut audio_models = HashSet::new();
    let mut video_models = HashSet::new();

    for model in &api_response.data {
        let Some(arch) = &model.architecture else {
            continue;
        };

        let has_image = arch.input_modalities.iter().any(|m| m == "image");
        let has_audio = arch.input_modalities.iter().any(|m| m == "audio");
        let has_video = arch.input_modalities.iter().any(|m| m == "video");
        let has_text = arch.input_modalities.iter().any(|m| m == "text");

        // Extract the short model name (after the provider prefix)
        let short_name = extract_short_name(&model.id);

        if has_image {
            vision_models.insert(short_name.clone());
        }

        if has_audio {
            audio_models.insert(short_name.clone());
        }

        if has_video {
            video_models.insert(short_name.clone());
        }

        // Text-only = has text but no image/audio/video
        if has_text && !has_image && !has_audio && !has_video {
            text_only_models.insert(short_name);
        }
    }

    // Sort for deterministic output
    let mut vision: Vec<_> = vision_models.into_iter().collect();
    let mut text_only: Vec<_> = text_only_models.into_iter().collect();
    let mut audio: Vec<_> = audio_models.into_iter().collect();

    vision.sort();
    text_only.sort();
    audio.sort();

    println!("Vision models: {}", vision.len());
    println!("Text-only models: {}", text_only.len());
    println!("Audio models: {}", audio.len());

    // Generate the Rust file
    let timestamp = chrono_lite_now();
    let content = generate_rust_file(&vision, &text_only, &audio, &timestamp);

    // Write to file
    let mut file = fs::File::create(OUTPUT_FILE)?;
    file.write_all(content.as_bytes())?;

    println!("Updated {}", OUTPUT_FILE);

    Ok(())
}

/// Extract the short model name from a full ID like "openai/gpt-4o".
fn extract_short_name(id: &str) -> String {
    // Keep the full ID for exact matching, but also extract short name
    // for pattern matching
    if let Some(pos) = id.rfind('/') {
        id[pos + 1..].to_lowercase()
    } else {
        id.to_lowercase()
    }
}

/// Simple timestamp without chrono dependency.
fn chrono_lite_now() -> String {
    use std::time::{SystemTime, UNIX_EPOCH};
    let duration = SystemTime::now()
        .duration_since(UNIX_EPOCH)
        .unwrap_or_default();
    let secs = duration.as_secs();

    // Convert to rough ISO format (good enough for documentation)
    let days_since_epoch = secs / 86400;
    let years = 1970 + days_since_epoch / 365;
    let remaining_days = days_since_epoch % 365;
    let months = remaining_days / 30 + 1;
    let days = remaining_days % 30 + 1;

    format!("{}-{:02}-{:02}T00:00:00Z", years, months, days)
}

fn generate_rust_file(
    vision: &[String],
    text_only: &[String],
    audio: &[String],
    timestamp: &str,
) -> String {
    let mut out = String::with_capacity(32_000);

    out.push_str(&format!(
        r#"//! Auto-generated model lists from OpenRouter API.
//!
//! DO NOT EDIT MANUALLY - This file is regenerated by the update-models binary.
//! Last updated: {}

/// Models that support vision/image input.
///
/// Source: OpenRouter API (input_modalities contains "image")
pub const VISION_MODELS: &[&str] = &[
"#,
        timestamp
    ));

    for model in vision {
        out.push_str(&format!("    \"{}\",\n", model));
    }
    out.push_str("];\n\n");

    out.push_str(
        r#"/// Models that are text-only (no vision/audio support).
///
/// Source: OpenRouter API (input_modalities is ["text"] only)
pub const TEXT_ONLY_MODELS: &[&str] = &[
"#,
    );

    for model in text_only {
        out.push_str(&format!("    \"{}\",\n", model));
    }
    out.push_str("];\n\n");

    out.push_str(
        r#"/// Models that support audio input.
///
/// Source: OpenRouter API (input_modalities contains "audio")
pub const AUDIO_MODELS: &[&str] = &[
"#,
    );

    for model in audio {
        out.push_str(&format!("    \"{}\",\n", model));
    }
    out.push_str("];\n\n");

    out.push_str(
        r#"/// All known models (union of all lists).
pub const ALL_MODELS: &[&str] = &[];
"#,
    );

    out
}
